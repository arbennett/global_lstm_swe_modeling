{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Background and data preparation\n",
    "\n",
    "### Outline\n",
    "\n",
    "* Imports\n",
    "* Brief discussion of xarray, intake, and zarr\n",
    "* Brief discussion of ERA5, SRTM, and CMIP6 + downscaling\n",
    "* Introduction to `torchdata.datapipes`\n",
    "* Walk through of data processing steps\n",
    "  * Subsetting to a region\n",
    "  * Scaling/normalizing the data\n",
    "  * Conversion between spatio-temporal dataset and ML-ready samples\n",
    "  * Data splitting for train-valid-test splits\n",
    "* Demonstration of the total pipeline and export to library code for next steps\n",
    "\n",
    "#### TODO\n",
    " * Add a collection of links for packages being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3PqH0Sr7myM",
    "outputId": "425949ac-3264-4144-d12d-220737c47422",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q zarr torchdata zen3geo dask[distributed] intake xarray fsspec aiohttp regionmask --upgrade\n",
    "!pip install -q git+https://github.com/carbonplan/cmip6-downscaling.git@1.0\n",
    "!pip install -q git+https://github.com/xarray-contrib/xbatcher.git@463546e7739e68b10f1ae456fb910a1628de1e5c\n",
    "!pip install -q jupyterlab-vim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jupyterlab_theme_solarized_dark\n",
    "!pip install jupyterlab_theme_sophon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670556560147
    },
    "id": "EyDDFpFn7z9o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import intake\n",
    "import regionmask\n",
    "import xbatcher\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import zen3geo\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from functools import partial\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from torchdata.datapipes.iter import IterDataPipe\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening the data\n",
    "\n",
    "TODO: Gonna need to clean this up and add some exposition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670555464985
    },
    "id": "ohF8jFzw8OdG"
   },
   "outputs": [],
   "source": [
    "def merge_data():\n",
    "    era5_daily_cat = intake.open_esm_datastore(\n",
    "        'https://cpdataeuwest.blob.core.windows.net/cp-cmip/training/ERA5-daily-azure.json'\n",
    "    )\n",
    "    met_files = sorted(list(era5_daily_cat.search(cf_variable_name='tasmax').df['zstore']))\n",
    "    years = np.arange(1985, 2015)\n",
    "    swe_files = [f'https://esiptutorial.blob.core.windows.net/eraswe/era5_raw_swe/era5_raw_swe_{year}.zarr'\n",
    "             for year in years]\n",
    "    swe_ds = xr.open_mfdataset(swe_files, engine='zarr')\n",
    "    daily_swe = swe_ds.resample(time='1D').mean().rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "    met_ds = xr.open_mfdataset(met_files,  engine='zarr')#.sel(time=swe_data['time'])\n",
    "    met_ds = met_ds.sel(time=slice(daily_swe['time'].min(), daily_swe['time'].max()))\n",
    "    met_ds['swe'] = daily_swe['sd']\n",
    "    mask = xr.open_dataset('https://esiptutorial.blob.core.windows.net/eraswe/mask_10k_household.zarr', engine='zarr')\n",
    "    terrain = xr.open_dataset('https://esiptutorial.blob.core.windows.net/eraswe/processed_slope_aspect_elevation.zarr', engine='zarr')\n",
    "    met_ds['mask'] = mask['sd'].rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "    met_ds = xr.merge([met_ds, terrain])\n",
    "    met_ds['mask'] = np.logical_and(~np.isnan(met_ds['elevation']), met_ds['mask']>0 ).astype(int)\n",
    "    return met_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1670557797268
    },
    "id": "I3CoB3HW8QdX",
    "outputId": "9f7a7817-bfd4-48ca-e22c-adbee29846b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = merge_data()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to `torchdata.datapipes`\n",
    "\n",
    "Before we dive into how we can process this data, let's take a quick side diversion into the `torchdata` package which  \n",
    "is designed to help with the creation and management of PyTorch datasets. Particularly, we'll use the `datapipes`that provides a set of tools for efficiently loading and transforming data. The key benefit of datapipe is its ability to process data in a streaming fashion, which can significantly reduce the memory usage and processing time of the data loading pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNumbersPipe(IterDataPipe):\n",
    "    \n",
    "    def __init__(self, sample_shape, number_samples):\n",
    "        super().__init__()\n",
    "        self.sample_shape = sample_shape\n",
    "        self.number_samples = number_samples\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.number_samples):\n",
    "            yield torch.randn(self.sample_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "randoms = RandomNumbersPipe(sample_shape=(5,2,), number_samples=3)\n",
    "for sample in randoms:\n",
    "    print(f'Shape: {sample.shape}, Mean: {torch.mean(sample)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datapipe approach makes it easy to apply transformation functions that can be used to preprocess data as it is loaded. These functions can be chained together to form a pipeline, allowing for complex data transformations to be performed with ease. We'll make use of this to go from the raw, gridded datasets to something that is usable by our model that we'll define in the next tutorial notebook. For now, let's suppose that we simply want to transpose the numbers that come out of the `RandomNumbersPipe` that we used earlier. This is easy enough to write as a simple function, which we call `transpose` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(x):\n",
    "    return x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in hand, we can simply call the `map` method on the `randoms` pipe, which produces a new datapipe that can be iterated over. This can easily be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randoms = RandomNumbersPipe(sample_shape=(5,2,), number_samples=3)\n",
    "transposed = randoms.map(transpose)\n",
    "\n",
    "for sample in transposed:\n",
    "    print(f'Shape: {sample.shape}, Mean: {torch.mean(sample)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The extract-transform-load (ETL) pipeline\n",
    "\n",
    "Now that we know how to access the raw data and have a basic strategy for manipulating said data into something that can ostensibly be used for training a recurrent neural network (RNN). Our basic starting point will be to subset the global data down to a region of interest. As mentioned in other parts of the tutorial, we take this approach simply to reduce the computational workload and make it easy to run this example in an end-to-end fashion in a timely manner. To do this we will first implement a new `RegionalSubsetterPipe` class which takes the full dataset and a list of regions from the regions defined in Giorgi and Francisco (2020) and dynamically select only one region at a time. In addition to the benefit of making it easy to run this tutorial on limited time/compute, this actually has another practical benefit for running global analyses - which is that most of the gridcells in the global domain actually are not located in areas that we have flagged for snow modeling via the `mask` variable. This regional subsetting means we will have far fewer samples to filter out at training time, which will lower not only the training time but the amount of data that is ultimately transferred over the wire and processed. This type of approach, where we are using \n",
    "publicly provided, large, \"analysis-ready\" datasets is very useful for reproducibility, accessiblility, proof-of-concept research, and learning materials. For larger projects and mature research programs, it usually will be better to actually process the data similarly to what we do here, but save the intermediate samples out to disk/cloud storage directly and avoid the computation associated with the sampling process.\n",
    "\n",
    "Anyhow, we can define the `RegionalSubsetterPipe` in a relatively straightforward manner, by taking in the full dataset, a sequence of the regions that are to be processed, and optionally a flag for whether to load an entire region into memory up front. The mechancs for actually selecting the region from the raw dataset is slightly involved, so we will walk through that piece in a bit more detail. First, here's the full class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionalSubsetterPipe(IterDataPipe):\n",
    "        \n",
    "    def __init__(self, ds, selected_regions=None, preload=False):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.selected_regions = self.to_sequence(selected_regions)\n",
    "        self.preload = preload\n",
    "        \n",
    "    def to_sequence(self, seq):\n",
    "        if isinstance(seq, str):\n",
    "            return (seq, )\n",
    "        return seq\n",
    "\n",
    "    def __iter__(self):\n",
    "        if not self.selected_regions:\n",
    "            yield self.ds\n",
    "        else:\n",
    "            for region in self.selected_regions:\n",
    "                self.selected_ds = select_region(self.ds, region)\n",
    "                if self.preload:\n",
    "                    self.selected_ds = self.selected_ds.load()\n",
    "                yield self.selected_ds\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you study the `RegionalSubsetterPipe` code closely, you'll note that it calls the `select_region` function inside of the `__iter__` method. This hasn't been defined or imported yet, so before we can actually use this we'll have to figure out a way to select a single region from a given `xarray` dataset. Luckily, most of the functionality for this has already been developed in the `regionmask` package which we installed in the first cell.\n",
    "\n",
    "Given that, we can define the function, which takes a dataset and the string name of a particular region from the `regionmask.defined_regions.ar6.land` attribute. You can look at that on your own if you're interested in what it contains, but for now we can just make a plot of all of the regions and their abbreviations so that you can easily select regions of interest. Note, in the given tutorial we will be assuming you are using the `WNA` region, but the code should be relatively straightforward enough to modify for other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionmask.defined_regions.ar6.land.plot(\n",
    "    label='abbrev', \n",
    "    text_kws={'fontsize': 5}, \n",
    "    line_kws={'linewidth':0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting back to the task at hand, we just need a function which takes a dataset and a region as a string and then subsets it to the given region. This can be done with `regionmask`'s built in functionality for masking. Given this we just have to find the regions which match the given mask, and then subset them. This is handled below, but we recommend you play with the steps individually if you want to learn how they work in-depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_region(ds, region): \n",
    "    # Get all regions & create mask from lat/lons\n",
    "    regions = regionmask.defined_regions.ar6.land\n",
    "    region_id_mask = regions.mask(ds['lon'], ds['lat'])\n",
    "    # Get unique listing of region names & abbreviations\n",
    "    reg = np.unique(region_id_mask.values)\n",
    "    reg = reg[~np.isnan(reg)]\n",
    "    region_abbrevs = np.array(regions[reg].abbrevs)\n",
    "    region_names = np.array(regions[reg].names)\n",
    "    # Create a mask that only contains the region of interest\n",
    "    selection_mask = 0.0 * region_id_mask.copy()\n",
    "    region_idx = np.argwhere(region_abbrevs == region)[0][0]\n",
    "    region_mask = (region_id_mask == reg[region_idx]).astype(int)\n",
    "    return ds.where(region_mask, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, one of the benefits of working in the data-pipe framework is we can iteratively develop and test each component of the ETL pipeline in a flexible and modular way that fits really nicely into the Jupyter workflow. To see this in action, let's actually instantiate and test if this behaves as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RegionalSubsetterPipe(ds, 'WNA')\n",
    "for subset in r:\n",
    "    print(subset.dims)\n",
    "    print(subset.coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is a subset of the oeverall dataset given the number of latitude and longitude values remaining, and their values tend to match up to what we want, but it's worth making sure things look reasonable before we move on to the rest of the processing pipeline. Here we just show a single map of the data for the first timeset, and can make out most of the major features of the western North America, including some of the coastal oceans which are included before masking things out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['tasmin'].isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have a simple and scalable first step to being able to process this data quickly, since this selection method is lazy and means we can reach this point with almost no actual underlying computation. The next step that is reasonable to ask is: given this as our modeling domain, what does an actual \"sample\" consist of? Since we are still dealing with a relatively coarse spatial scale we will assume that we can neglect spatial redistribution of snow via wind and other processes. But, because snow accumulation and ablation processes can occur over long time periods we will need to account for time explicitly. That means that we will consider a single sample to be a single grid cell with some time history. We'll get a bit more into how this is actually represented in the model later, but for now we can summarize that we want to select out a single location from the model for a specified period of time. We will use the `xbatcher` python package to actually facilitate this. \n",
    "\n",
    "Given we have the `zen3geo` package also installed from the setup cells `xbatcher` and the `torchdata.datapipes` are already interoperable via the `pipe.slice_with_xbatcher` method.  We simply have to define some dimensions to consider a sample and we are good to go. But, before we do that, it's worth taking a moment to discuss samples versus batches. A sample is considered an individual example of the mapping that we want the model to learn. Ideally we could process all samples simultaneously to optimize our model, but as in many other areas where machine learning is common, this is computationally intractable for use. As an alternative to \"full batch\" processing we use the now standart approach of \"mini batch\" processing where we group together a small fraction of the total samples available to provide to the model and optimization routine at each update step. This is the reason that learning across large datasets is possible, and we implement this in a simple manner by grouping together nearby gridcells - often referred to as \"chunks\", \"patches\", or most commonly in the geospatial community as \"chips\". \n",
    "\n",
    "This is all handles behind the scenes by `xbatcher` simply by specifying the `batch_dims`. The time period that we consider relevant is specified via the `input_dims` argument. We consider this to be 180 days here as a \"naive\" choice because we have chosen our gridcells of interest to be locations where snow is common, but not present year-round in the ERA5 data. This is a \"hyperparameter\" that is ripe for further testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims={'time': 180}\n",
    "batch_dims={'lat': 30, 'lon': 30}\n",
    "input_overlap={'time': 45}\n",
    "\n",
    "pipe = RegionalSubsetterPipe(ds, ['WNA'])\n",
    "pipe = pipe.slice_with_xbatcher(\n",
    "    input_dims=input_dims,\n",
    "    batch_dims=batch_dims,\n",
    "    input_overlap=input_overlap,\n",
    "    preload_batch=False\n",
    ")\n",
    "for batch in pipe:\n",
    "    b = batch\n",
    "    break\n",
    "    \n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this you can see that `xbatcher` has automatically flattened out the latitudes and longitudes into a single `sample` dimension. But, given that our selection from the `regionmask` utility is a square bounding box over the full dataset we still have to consider if all of the data in the `sample` dimension is valid. We will filter this data out with a function that removes any gridcells that do not lie within our predefined mask. This ends up being a simple boolean mask check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch):\n",
    "    return batch.where(batch['mask']>0, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then incorporate this into our data processing pipeline simply by calling the `.map` method on our existing pipeline objects with this function as the argument. We will hold off on demonstrating this until we have completed the last two steps of the pipeline, but feel free to experiment with the final code we provide to see how this works in practice.\n",
    "\n",
    "The next question is, given a batch of data to be fed into a model, do we need to do any \"postprocessing\" first? Generally it is necessary to scale data to be approximately normalized for deep-learning based models to train effectively. This is no exception in Earth/environmental science applications where inputs/covariates can often span multiple orders of magnitude. We'll use the basic standardization technique where we subtract the mean of the data and divide by the standard deviation for each variable. This sits atop an assumption that our data is somewhat normally distributed, which is certainly not true for all variables but we can get around by making some deliberate choices for scale factors. This would be a ripe area for further exploration, and can result in some nice boosts to both model training speed and end model performance at the end of the day, but can get a bit complex for our purposes here so we leave improvements and tweakes as next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670555464985
    },
    "id": "ohF8jFzw8OdG"
   },
   "outputs": [],
   "source": [
    "def transform_batch(batch):\n",
    "    scale_means = xr.Dataset()\n",
    "    scale_means['mask'] = 0.0\n",
    "    scale_means['swe'] = 0.0\n",
    "    scale_means['pr'] = 0.00\n",
    "    scale_means['tasmax'] = 295.0\n",
    "    scale_means['tasmin'] = 280.0\n",
    "    scale_means['elevation'] = 630.0\n",
    "    scale_means['aspect_cosine'] = 0.0\n",
    "    \n",
    "    scale_stds = xr.Dataset()\n",
    "    scale_stds['mask'] = 1.0\n",
    "    scale_stds['swe'] = 3.0\n",
    "    scale_stds['pr'] = 1/100.0\n",
    "    scale_stds['tasmax'] = 80.0\n",
    "    scale_stds['tasmin'] = 80.0\n",
    "    scale_stds['elevation'] = 830.0\n",
    "    scale_stds['aspect_cosine'] = 1.0\n",
    "    # Just do a simple standardization\n",
    "    batch = (batch - scale_means) / scale_stds\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice - we're almost there! Last thing we have to do is actually split the data out into our inputs/outputs. This point is where we have to actually come face-to-face with the data and finally get it into the format that the ML model expects.\n",
    "\n",
    "At this point we've got a data pipeline that can open the data, slice it up into samples, filter out missing samples, and normalize the data. This is all done as an `xarray.DataSet`, which will first need to be stacked together so all of the variables are part of a single array, and then we will need to make sure that the dimension order matches what our model will expect. For the moment that step will be a leap of faith, but you will see how things pan out in the next portion of the tutorial. Finally, we just need to convert things to a `torch.tensor` so that we can actually take advantage of the PyTorch ecosystem. As a final note, we have implemented things so that if the number of samples is less than some threshold we just skip that batch. This is because our mask was irregular and it's generally not worth running very small samples through our model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670555464985
    },
    "id": "ohF8jFzw8OdG"
   },
   "outputs": [],
   "source": [
    "def stack_split_convert(\n",
    "    batch, \n",
    "    in_vars, \n",
    "    out_vars, \n",
    "    in_selectors={},\n",
    "    out_selectors={},\n",
    "    min_samples=200\n",
    "):\n",
    "    dims = ('sample', 'time', 'variable')\n",
    "    if len(batch['sample']) > min_samples:\n",
    "        # Go from a dataset which has multiple variables\n",
    "        # to a single dataarray, which stacks the variables.\n",
    "        # Then, transpose them to the desired order for the \n",
    "        # model in the next step.\n",
    "        x = (batch[in_vars]\n",
    "              .to_array()\n",
    "              .transpose(*dims))\n",
    "        y = (batch[out_vars]\n",
    "              .to_array()\n",
    "              .transpose(*dims))\n",
    "        # Convert to `torch.tensor`\n",
    "        # The call `x.values` converts from xarray to numpy\n",
    "        # Then following that we convert to the tensor\n",
    "        # Finally we make sure that we are using 32 bit floats\n",
    "        x = torch.tensor(x.values).float()\n",
    "        y = torch.tensor(y.values).float()\n",
    "    else:\n",
    "        # Just return an empty tensor so we can skip later\n",
    "        x, y = torch.tensor([]), torch.tensor([])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, note that the `stack_split_convert` function takes many parameters which can be flexibly specified to change how the data comes out. Because pytorch data pipes can only be functions of a single variable, we will need to predefine some of these, and then use a handy tool from the `functools` module called `partial`. \n",
    "\n",
    "To explain the `partial` function in a concise manner, let's just go over a simple example. Imagine you have inherited a two argument function aptly named `two_arg_fun` from a colleague:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_arg_fun(a, b):\n",
    "    return a ** b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And further suppose that you know what `b` is for your particular case, and it's a fixed value of 2. Then, it's not terribly difficult to run this once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_b = 2\n",
    "x = np.arange(0, 5, 0.05)\n",
    "y = two_arg_fun(x, b=fixed_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, where `partial` comes in is making it easy to define a new function that not only has a name, but can be passed around to other functions. For example, in our very simple case we can reproduce the prevous behavior with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, dpi=150, figsize=(5,2))\n",
    "fixed_fun = partial(two_arg_fun, b=fixed_b)\n",
    "ax.plot(x, y, label='original')\n",
    "ax.plot(x, fixed_fun(x), label='with partial', linestyle=':', linewidth=3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that small, practical example let's get back to our final piece of the data pipeline. We just need to be able to inject variables into the `stack_split_convert` function. We can define all of them up front, which acts as a nice \"configuration\" step as we'll see later. This will define all of the important things for our training workflow like what the inputs and targets/outputs are, sequence lengths, dimensions of our batch sizes, and how much to overlap training samples. With everything configured we can just run it through the `partial` function with relevant keywords and then we are good to put things together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1670557797268
    },
    "id": "I3CoB3HW8QdX",
    "outputId": "9f7a7817-bfd4-48ca-e22c-adbee29846b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_vars = ['pr',  'tasmax',  'tasmin',  'elevation',  'aspect_cosine']\n",
    "out_vars = ['swe']\n",
    "output_sequence_length = 1\n",
    "output_selector = {'time': slice(-output_sequence_length, None)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1670557797268
    },
    "id": "I3CoB3HW8QdX",
    "outputId": "9f7a7817-bfd4-48ca-e22c-adbee29846b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "convert = partial(\n",
    "    stack_split_convert, \n",
    "    in_vars=in_vars, \n",
    "    out_vars=out_vars, \n",
    "    out_selectors=output_selector,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, it's been a bit of work to get here, but that's all of the components for our data processing pipeline. Now we just need to be able to assemble it. We promised that the point of using `torchdata.datapipes` was to simplify things and now you finally get to see that in action! We have a bit more configuration set up just for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['WNA']\n",
    "varlist = ['mask'] + in_vars + out_vars\n",
    "input_sequence_length = 180  \n",
    "input_dims={'time': input_sequence_length}\n",
    "batch_dims={'lat': 30, 'lon': 30}\n",
    "input_overlap={'time': 45}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can now actually chain things together and try it out. This is a crucial step in developing your own data pipelines, so make sure to see how each of the above steps is incorporated into the final pipeline of steps below. You can (and should) verify that the pipeline works before you step through to the next notebook, but that's simple enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the data and subset to region\n",
    "dp = RegionalSubsetterPipe(ds[varlist], regions)\n",
    "# Generate chips from the region\n",
    "dp = dp.slice_with_xbatcher(\n",
    "    input_dims=input_dims,\n",
    "    batch_dims=batch_dims,\n",
    "    preload_batch=False\n",
    ")\n",
    "# Filter out any missing data\n",
    "dp = dp.map(filter_batch)\n",
    "# Transform/normalize the data\n",
    "dp = dp.map(transform_batch)\n",
    "# Reshape and convert it to a torch.tensor\n",
    "dp = dp.map(convert)\n",
    "\n",
    "x, y = next(iter(dp))\n",
    "print('Input batch has shape:  ', x.shape)\n",
    "print('Target batch has shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the data pipeline creation\n",
    "\n",
    "Now that we have a full workflow in place it is good practice to wrap things up into a nice function so that it can be used in future workflows/scripts/notebooks. For completeness we show how to wrap all of this up, and also show how this can be used as a library function. This is all implemented in the `src.datapipes` module which will be imported in our training routines and beyond, so make sure to look at how the implementations are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_pipeline(\n",
    "    ds, \n",
    "    regions, \n",
    "    input_vars, \n",
    "    output_vars,\n",
    "    input_sequence_length,\n",
    "    output_sequence_length,\n",
    "    batch_dims,\n",
    "    input_overlap,\n",
    "):\n",
    "    # Preamble: just set some stuff up\n",
    "    output_selector = {'time': slice(-output_sequence_length, None)}\n",
    "    input_dims={'time': input_sequence_length}\n",
    "    varlist = ['mask'] + input_vars + output_vars\n",
    "    convert = partial(\n",
    "        stack_split_convert, \n",
    "        in_vars=input_vars, \n",
    "        out_vars=output_vars, \n",
    "        out_selectors=output_selector,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    # Chain together the datapipe\n",
    "    dp = RegionalSubsetterPipe(ds[varlist], selected_regions=regions)\n",
    "    dp = dp.slice_with_xbatcher(\n",
    "        input_dims=input_dims,\n",
    "        batch_dims=batch_dims,\n",
    "        input_overlap=input_overlap,\n",
    "        preload_batch=False\n",
    "    )\n",
    "    dp = dp.map(filter_batch)\n",
    "    dp = dp.map(transform_batch)\n",
    "    dp = dp.map(convert)   \n",
    "    return dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step you can see how we can instantiate the entire workflow pipeline and finally iterate over it. This provides the baseline for the model that we'll train, and is a great achievement to have completed. Frankly, setting up the data processing pipeline can be one of the most onerous and tricky parts of a machine learning pipeline. Make sure that you spend appropriate time here before diving straight into trying to train/fit your model because, as they say, \"garbage in = garbage out\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = make_data_pipeline(\n",
    "    ds, ['WNA'], in_vars, out_vars,\n",
    "    input_sequence_length, output_sequence_length,\n",
    "    batch_dims, input_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(p))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
