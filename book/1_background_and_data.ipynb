{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Background and data preparation\n",
    "\n",
    "### Outline\n",
    "\n",
    "* Imports\n",
    "* Brief discussion of xarray, intake, and zarr\n",
    "* Brief discussion of ERA5, SRTM, and CMIP6 + downscaling\n",
    "* Introduction to `torchdata.datapipes`\n",
    "* Walk through of data processing steps\n",
    "  * Subsetting to a region\n",
    "  * Scaling/normalizing the data\n",
    "  * Conversion between spatio-temporal dataset and ML-ready samples\n",
    "  * Data splitting for train-valid-test splits\n",
    "* Demonstration of the total pipeline and export to library code for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3PqH0Sr7myM",
    "outputId": "425949ac-3264-4144-d12d-220737c47422",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cmip6-preprocessing 0.6.0 requires pint-xarray, which is not installed.\n",
      "s3fs 2022.8.2 requires fsspec==2022.8.2, but you have fsspec 2023.1.0 which is incompatible.\n",
      "msal-extensions 0.3.0 requires portalocker~=1.0; platform_system != \"Windows\", but you have portalocker 2.7.0 which is incompatible.\n",
      "intake-esm 2021.8.17 requires h5netcdf>=0.8.1, but you have h5netcdf 0.0.0 which is incompatible.\n",
      "geogif 0.1.3 requires dask[delayed]<2023,>=2021.4.1, but you have dask 2023.2.1 which is incompatible.\n",
      "gcsfs 2022.8.2 requires fsspec==2022.8.2, but you have fsspec 2023.1.0 which is incompatible.\n",
      "dask-cuda 0+untagged.46.g68c7e93.dirty requires dask==2022.03.0, but you have dask 2023.2.1 which is incompatible.\n",
      "dask-cuda 0+untagged.46.g68c7e93.dirty requires distributed==2022.03.0, but you have distributed 2023.2.1 which is incompatible.\n",
      "cmip6-preprocessing 0.6.0 requires xgcm<0.7.0, but you have xgcm 0.8.0 which is incompatible.\n",
      "azure-cli-telemetry 1.0.2 requires applicationinsights<0.11.8,>=0.11.1, but you have applicationinsights 0.11.9 which is incompatible.\n",
      "azure-cli-telemetry 1.0.2 requires portalocker==1.2.1, but you have portalocker 2.7.0 which is incompatible.\n",
      "azure-cli-core 2.0.61 requires knack>=0.5.3, but you have knack 0.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.5.1 requires portalocker>=2.0.0, but you have portalocker 1.7.1 which is incompatible.\n",
      "azure-cli-telemetry 1.0.2 requires applicationinsights<0.11.8,>=0.11.1, but you have applicationinsights 0.11.9 which is incompatible.\n",
      "azure-cli-telemetry 1.0.2 requires portalocker==1.2.1, but you have portalocker 1.7.1 which is incompatible.\n",
      "azure-cli-core 2.0.61 requires knack>=0.5.3, but you have knack 0.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q zarr torchdata zen3geo dask[distributed] intake xarray fsspec aiohttp regionmask --upgrade\n",
    "!pip install -q git+https://github.com/carbonplan/cmip6-downscaling.git@1.0\n",
    "!pip install -q git+https://github.com/xarray-contrib/xbatcher.git@463546e7739e68b10f1ae456fb910a1628de1e5c\n",
    "!pip install -q jupyterlab-vim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670556560147
    },
    "id": "EyDDFpFn7z9o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import intake\n",
    "import regionmask\n",
    "import xbatcher\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import zen3geo\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from functools import partial\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from torchdata.datapipes.iter import IterDataPipe\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening the data\n",
    "\n",
    "TODO: Gonna need to clean this up and add some exposition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670555464985
    },
    "id": "ohF8jFzw8OdG"
   },
   "outputs": [],
   "source": [
    "def merge_data():\n",
    "    era5_daily_cat = intake.open_esm_datastore(\n",
    "        'https://cpdataeuwest.blob.core.windows.net/cp-cmip/training/ERA5-daily-azure.json'\n",
    "    )\n",
    "    met_files = sorted(list(era5_daily_cat.search(cf_variable_name='tasmax').df['zstore']))\n",
    "    years = np.arange(1985, 2015)\n",
    "    swe_files = [f'https://esiptutorial.blob.core.windows.net/eraswe/era5_raw_swe/era5_raw_swe_{year}.zarr'\n",
    "             for year in years]\n",
    "    swe_ds = xr.open_mfdataset(swe_files, engine='zarr')\n",
    "    daily_swe = swe_ds.resample(time='1D').mean().rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "    met_ds = xr.open_mfdataset(met_files,  engine='zarr')#.sel(time=swe_data['time'])\n",
    "    met_ds = met_ds.sel(time=slice(daily_swe['time'].min(), daily_swe['time'].max()))\n",
    "    met_ds['swe'] = daily_swe['sd']\n",
    "    mask = xr.open_dataset('https://esiptutorial.blob.core.windows.net/eraswe/mask_10k_household.zarr', engine='zarr')\n",
    "    terrain = xr.open_dataset('https://esiptutorial.blob.core.windows.net/eraswe/processed_slope_aspect_elevation.zarr', engine='zarr')\n",
    "    met_ds['mask'] = mask['sd'].rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "    met_ds = xr.merge([met_ds, terrain])\n",
    "    met_ds['mask'] = np.logical_and(~np.isnan(met_ds['elevation']), met_ds['mask']>0 ).astype(int)\n",
    "    return met_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1670557797268
    },
    "id": "I3CoB3HW8QdX",
    "outputId": "9f7a7817-bfd4-48ca-e22c-adbee29846b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = merge_data()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionmask.defined_regions.ar6.land.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to `torchdata.datapipes`\n",
    "\n",
    "TODO: 2 parts here - why, and how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNumbersPipe(IterDataPipe):\n",
    "    \n",
    "    def __init__(self, sample_shape, number_samples):\n",
    "        super().__init__()\n",
    "        self.sample_shape = sample_shape\n",
    "        self.number_samples = number_samples\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.number_samples):\n",
    "            yield torch.randn(self.sample_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "randoms = RandomNumbersPipe(sample_shape=(5,2,), number_samples=3)\n",
    "for sample in randoms:\n",
    "    print(f'Shape: {sample.shape}, Mean: {torch.mean(sample)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(x):\n",
    "    return x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randoms = RandomNumbersPipe(sample_shape=(5,2,), number_samples=3)\n",
    "transposed = randoms.map(transpose)\n",
    "\n",
    "for sample in transposed:\n",
    "    print(f'Shape: {sample.shape}, Mean: {torch.mean(sample)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The extract-transform-load (ETL) pipeline\n",
    "\n",
    "Now that we know how to access the raw data and have a basic strategy for manipulating said data into something that can ostensibly be used for training a recurrent neural network (RNN). Our basic starting point will be to subset the global data down to a region of interest. As mentioned in other parts of the tutorial, we take this approach simply to reduce the computational workload and make it easy to run this example in an end-to-end fashion in a timely manner. To do this we will first implement a new `RegionalSubsetterPipe` class which takes the full dataset and a list of regions from the regions defined in Giorgi and Francisco (2020) and dynamically select only one region at a time. In addition to the benefit of making it easy to run this tutorial on limited time/compute, this actually has another practical benefit for running global analyses - which is that most of the gridcells in the global domain actually are not located in areas that we have flagged for snow modeling via the `mask` variable. This regional subsetting means we will have far fewer samples to filter out at training time, which will lower not only the training time but the amount of data that is ultimately transferred over the wire and processed. This type of approach, where we are using \n",
    "publicly provided, large, \"analysis-ready\" datasets is very useful for reproducibility, accessiblility, proof-of-concept research, and learning materials. For larger projects and mature research programs, it usually will be better to actually process the data similarly to what we do here, but save the intermediate samples out to disk/cloud storage directly and avoid the computation associated with the sampling process.\n",
    "\n",
    "Anyhow, we can define the `RegionalSubsetterPipe` in a relatively straightforward manner, by taking in the full dataset, a sequence of the regions that are to be processed, and optionally a flag for whether to load an entire region into memory up front. The mechancs for actually selecting the region from the raw dataset is slightly involved, so we will walk through that piece in a bit more detail. First, here's the full class:\n",
    "\n",
    "TODO: Further explanation of how the `select_region` method works...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670555464985
    },
    "id": "ohF8jFzw8OdG"
   },
   "outputs": [],
   "source": [
    "class RegionalSubsetterPipe(IterDataPipe):\n",
    "        \n",
    "    def __init__(self, ds, selected_regions, preload=False):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.selected_regions = self.to_sequence(selected_regions)\n",
    "        self.preload = preload\n",
    "        \n",
    "    def to_sequence(self, seq):\n",
    "        if isinstance(seq, str):\n",
    "            return (seq, )\n",
    "        return seq\n",
    "    \n",
    "    def select_region(self, region): \n",
    "        # Get all regions & create mask from lat/lons\n",
    "        regions = regionmask.defined_regions.ar6.land\n",
    "        region_id_mask = regions.mask(ds['lon'], ds['lat'])\n",
    "        # Get unique listing of region names & abbreviations\n",
    "        reg = np.unique(region_id_mask.values)\n",
    "        reg = reg[~np.isnan(reg)]\n",
    "        region_abbrevs = np.array(regions[reg].abbrevs)\n",
    "        region_names = np.array(regions[reg].names)\n",
    "        # Create a mask that only contains the region of interest\n",
    "        selection_mask = 0.0 * region_id_mask.copy()\n",
    "        region_idx = np.argwhere(region_abbrevs == region)[0][0]\n",
    "        region_mask = (region_id_mask == region_idx).astype(int)\n",
    "        return self.ds.where(region_mask, drop=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for region in self.selected_regions:\n",
    "            self.selected_ds = self.select_region(region)\n",
    "            if self.preload:\n",
    "                self.selected_ds = self.selected_ds.load()\n",
    "            yield self.selected_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, one of the benefits of working in the data-pipe framework is we can iteratively develop and test each component of the ETL pipeline in a flexible and modular way that fits really nicely into the Jupyter workflow. To see this in action, let's actually instantiate and test if this behaves as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RegionalSubsetterPipe(ds, 'WNA')\n",
    "for subset in r:\n",
    "    print(subset.dims)\n",
    "    print(subset.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['tasmin'].isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting out just the \"Western North America\" region (code WNA) we can see that this did indeed work. Selecting this region out yields a much smaller number of latitudes and longitudes, located exactly where we expect them. This is a good, and scalable first step to being able to process this data quickly, since this selection method is lazy and means we can reach this point with almost no actual underlying computation.\n",
    "\n",
    "So, the next step that is reasonable to ask is: given this as our modeling domain, what does an actual \"sample\" consist of? Since we are still dealing with a relatively coarse spatial scale we will assume that we can neglect spatial redistribution of snow via wind and other processes. But, because snow accumulation and ablation processes can occur over long time periods we will need to account for time explicitly. That means that we will consider a single sample to be a single grid cell with some time history. We'll get a bit more into how this is actually represented in the model later, but for now we can summarize that we want to select out a single location from the model for a specified period of time. We will use the `xbatcher` python package to actually facilitate this. \n",
    "\n",
    "Given we have the `zen3geo` package also installed from the setup cells `xbatcher` and the `torchdata.datapipes` are already interoperable via the `pipe.slice_with_xbatcher` method.  We simply have to define some dimensions to consider a sample and we are good to go. But, before we do that, it's worth taking a moment to discuss samples versus batches. A sample is considered an individual example of the mapping that we want the model to learn. Ideally we could process all samples simultaneously to optimize our model, but as in many other areas where machine learning is common, this is computationally intractable for use. As an alternative to \"full batch\" processing we use the now standart approach of \"mini batch\" processing where we group together a small fraction of the total samples available to provide to the model and optimization routine at each update step. This is the reason that learning across large datasets is possible, and we implement this in a simple manner by grouping together nearby gridcells - often referred to as \"chunks\", \"patches\", or most commonly in the geospatial community as \"chips\". \n",
    "\n",
    "This is all handles behind the scenes by `xbatcher` simply by specifying the `batch_dims`. The time period that we consider relevant is specified via the `input_dims` argument. We consider this to be 180 days here as a \"naive\" choice because we have chosen our gridcells of interest to be locations where snow is common, but not present year-round in the ERA5 data. This is a \"hyperparameter\" that is ripe for further testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims={'time': 180}\n",
    "batch_dims={'lat': 30, 'lon': 30}\n",
    "input_overlap={'time': 45}\n",
    "\n",
    "pipe = RegionalSubsetterPipe(ds, ['WNA'])\n",
    "pipe = pipe.slice_with_xbatcher(\n",
    "    input_dims=input_dims,\n",
    "    batch_dims=batch_dims,\n",
    "    input_overlap=input_overlap,\n",
    "    preload_batch=False\n",
    ")\n",
    "for batch in pipe:\n",
    "    b = batch\n",
    "    break\n",
    "    \n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this you can see that `xbatcher` has automatically flattened out the latitudes and longitudes into a single `sample` dimension. But, given that our selection from the `regionmask` utility is a square bounding box over the full dataset we still have to consider if all of the data in the `sample` dimension is valid. We will filter this data out with a function that removes any gridcells that do not lie within our predefined mask. This ends up being a simple boolean mask check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch):\n",
    "    return batch.where(batch['mask']>0, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then incorporate this into our data processing pipeline simply by calling the `.map` method on our existing pipeline objects with this function as the argument. We will hold off on demonstrating this until we have completed the last two steps of the pipeline, but feel free to experiment with the final code we provide to see how this works in practice.\n",
    "\n",
    "The next question is, given a batch of data to be fed into a model, do we need to do any \"postprocessing\" first? Generally it is necessary to scale data to be approximately normalized for deep-learning based models to train effectively. This is no exception in Earth/environmental science applications where inputs/covariates can often span multiple orders of magnitude. We'll use the basic standardization technique where we subtract the mean of the data and divide by the standard deviation for each variable. This sits atop an assumption that our data is somewhat normally distributed, which is certainly not true for all variables but we can get around by making some deliberate choices for scale factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670555464985
    },
    "id": "ohF8jFzw8OdG"
   },
   "outputs": [],
   "source": [
    "def transform_batch(batch):\n",
    "    scale_means = xr.Dataset()\n",
    "    scale_means['mask'] = 0.0\n",
    "    scale_means['swe'] = 0.0\n",
    "    scale_means['pr'] = 0.00\n",
    "    scale_means['tasmax'] = 295.0\n",
    "    scale_means['tasmin'] = 280.0\n",
    "    scale_means['elevation'] = 630.0\n",
    "    scale_means['aspect_cosine'] = 0.0\n",
    "    \n",
    "    scale_stds = xr.Dataset()\n",
    "    scale_stds['mask'] = 1.0\n",
    "    scale_stds['swe'] = 3.0\n",
    "    scale_stds['pr'] = 1/100.0\n",
    "    scale_stds['tasmax'] = 80.0\n",
    "    scale_stds['tasmin'] = 80.0\n",
    "    scale_stds['elevation'] = 830.0\n",
    "    scale_stds['aspect_cosine'] = 1.0\n",
    "    \n",
    "    batch = (batch - scale_means) / scale_stds\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice - we're almost there! Last thing we have to do is actually split the data out into our inputs/outputs. This point is where we have to actually come face-to-face with the data and finally get it into the format that the ML model expects.\n",
    "\n",
    "TODO: Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670555464985
    },
    "id": "ohF8jFzw8OdG"
   },
   "outputs": [],
   "source": [
    "def stack_split_convert(\n",
    "    batch, \n",
    "    in_vars, \n",
    "    out_vars, \n",
    "    in_selectors={},\n",
    "    out_selectors={},\n",
    "    min_samples=200\n",
    "):\n",
    "    if len(batch['sample']) > min_samples:\n",
    "        x = (batch[in_vars]\n",
    "                 .to_array()\n",
    "                 .transpose('sample', 'time', 'variable')\n",
    "                 .isel(**in_selectors))\n",
    "        y = (batch[out_vars]\n",
    "                  .to_array()\n",
    "                  .transpose('sample', 'time', 'variable')\n",
    "                  .isel(**out_selectors))\n",
    "        x = torch.tensor(x.values).float()\n",
    "        y = torch.tensor(y.values).float()\n",
    "        if device:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "    else:\n",
    "        x, y = torch.tensor([]), torch.tensor([])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, note that the `stack_split_convert` function takes many parameters which can be flexibly specified to change how the data comes out. Because pytorch data pipes can only be functions of a single variable, we will need to predefine some of these, and then use a handy tool from the `functools` module called `partial`. \n",
    "\n",
    "<! EXPLAIN PARTIAL FUNCTION APPLICATION HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1670557797268
    },
    "id": "I3CoB3HW8QdX",
    "outputId": "9f7a7817-bfd4-48ca-e22c-adbee29846b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_vars = ['pr',  'tasmax',  'tasmin',  'elevation',  'aspect_cosine']\n",
    "out_vars = ['swe']\n",
    "varlist = ['mask'] + in_vars + out_vars\n",
    "input_sequence_length = 180  \n",
    "output_sequence_length = 1\n",
    "output_selector = {'time': slice(-output_sequence_length, None)}\n",
    "input_dims={'time': input_sequence_length}\n",
    "batch_dims={'lat': 30, 'lon': 30}\n",
    "input_overlap={'time': 45}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1670557797268
    },
    "id": "I3CoB3HW8QdX",
    "outputId": "9f7a7817-bfd4-48ca-e22c-adbee29846b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "convert = partial(\n",
    "    stack_split_convert, \n",
    "    in_vars=in_vars, \n",
    "    out_vars=out_vars, \n",
    "    out_selectors=output_selector,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = RegionalSubsetterPipe(\n",
    "    ds[varlist].sel(time=slice('1985', '2015')).astype(np.float32),\n",
    "    selected_regions=regions,\n",
    ")\n",
    "dp = dp.slice_with_xbatcher(\n",
    "    input_dims=input_dims,\n",
    "    batch_dims=batch_dims,\n",
    "    preload_batch=False\n",
    ")\n",
    "dp = dp.map(filter_batch)\n",
    "dp = dp.map(transform_batch)\n",
    "dp = dp.map(convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating the data pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_pipeline(\n",
    "    ds, \n",
    "    regions, \n",
    "    input_vars, \n",
    "    output_vars,\n",
    "    input_sequence_length,\n",
    "    output_sequence_length,\n",
    "    batch_dims,\n",
    "    input_overlap,\n",
    "):\n",
    "    # Preamble: just set some stuff up\n",
    "    output_selector = {'time': slice(-output_sequence_length, None)}\n",
    "    input_dims={'time': input_sequence_length}\n",
    "    varlist = ['mask'] + input_vars + output_vars\n",
    "    convert = partial(\n",
    "        stack_split_convert, \n",
    "        in_vars=input_vars, \n",
    "        out_vars=output_vars, \n",
    "        out_selectors=output_selector,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    # Chain together the datapipe\n",
    "    dp = RegionalSubsetterPipe(ds[varlist], selected_regions=regions)\n",
    "    dp = dp.slice_with_xbatcher(\n",
    "        input_dims=input_dims,\n",
    "        batch_dims=batch_dims,\n",
    "        input_overlap=input_overlap,\n",
    "        preload_batch=False\n",
    "    )\n",
    "    dp = dp.map(filter_batch)\n",
    "    dp = dp.map(transform_batch)\n",
    "    dp = dp.map(convert)   \n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data_pipeline(\n",
    "    ds, ['WNA'], in_vars, out_vars,\n",
    "    input_sequence_length, output_sequence_length,\n",
    "    batch_dims, input_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
