{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0422ddf2-cefe-416d-8a8c-dc48c1e1ef67",
   "metadata": {},
   "source": [
    "# Tutorial 4: Running the model on downscaled climate projections\n",
    "\n",
    "### Outline:\n",
    "\n",
    "* Imports, including library code from previous steps\n",
    "* More in-depth exploration of the downscaled CMIP6 data\n",
    "* Loading the trained model\n",
    "* Setting up an inference pipeline for multiple projections\n",
    "* Analysis\n",
    "\n",
    "## Setup and configuration\n",
    "\n",
    "At this point you should be familiar with the setup routine, importing packages and setting devices and datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb38e5-770d-43d8-ab27-5398539159ea",
   "metadata": {
    "gather": {
     "logged": 1670556560147
    },
    "id": "EyDDFpFn7z9o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import intake\n",
    "import regionmask\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from torch import nn\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from src.datapipes import (\n",
    "    get_static_data, \n",
    "    select_region, \n",
    "    make_data_pipeline, \n",
    "    scale_means, \n",
    "    scale_stds\n",
    ")\n",
    "from src.utils import load_experiment\n",
    "from src.models import create_lstm_model\n",
    "from src.inference import (\n",
    "    unmask,\n",
    "    gen_inference_data_pipeline,\n",
    "    run_model\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745ca05-071f-4eb4-8fec-790215db8c61",
   "metadata": {},
   "source": [
    "## Getting the model imported and loaded up\n",
    "\n",
    "Just as we did in the previous portion of the tutorial, we will use the experiment file to load in our hyperparameters and settings. From there we can use the `create_lstm_model` to construct the structure. To this model structure we append a final `ReLU` function, which simply gets rid of the negative values that we saw before. The `LeakyReLU` used during training did make it so that these values were never too large, but at this point we can just chop them off. Next we load the weights, and set the device and data type appropriately. Finally, we set the model to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f74c33-c45e-42af-892e-9997a1ba6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '../experiments/tutorial/tutorial.yml'\n",
    "config = load_experiment(config_file)\n",
    "model = create_lstm_model(**config['model_config'])\n",
    "model = model.append(nn.ReLU())\n",
    "model.load_state_dict(torch.load(config['weights_file'], map_location=DEVICE))\n",
    "model.to(DEVICE).to(DTYPE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae23ec-1a70-47b9-8183-ba119a249502",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Unfortunately, we stil also have a bit of work to do to make the climate projections usable with our model. Now that we're running in forward mode on completely new data it's worth seeing what everything looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf6ab6-5668-4bcd-b294-6b06c2019620",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = intake.open_esm_datastore(\n",
    "  'https://cpdataeuwest.blob.core.windows.net/cp-cmip/version1/catalogs/global-downscaled-cmip6.json'\n",
    ")\n",
    "# cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a988e1-4dd9-4c78-a824-eb0667b3ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data = cat.search(timescale='day', variable_id='pr')\n",
    "# pr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2e296-7d43-4774-afdc-6255532369db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat = intake.open_esm_datastore(\n",
    "  'https://cpdataeuwest.blob.core.windows.net/cp-cmip/version1/catalogs/global-downscaled-cmip6.json'\n",
    ")\n",
    "\n",
    "cat_subset = cat.search(\n",
    "    method=\"GARD-SV\",\n",
    "    source_id=\"CanESM5\",\n",
    "    experiment_id=\"ssp245\",\n",
    "    variable_id=['tasmin', 'tasmax', 'pr'],\n",
    "    timescale='day',\n",
    ")\n",
    "dsets = cat_subset.to_dataset_dict()\n",
    "met_ds = list(dsets.values())[0]#.chunk({'time': 1, 'lat': 48, 'lon': 48})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714374f-0e79-4a65-afd7-9873388c5374",
   "metadata": {},
   "source": [
    "Next we can open up the static data, which contains the elevation, slopes, aspects, and mask variables. These are actually on a slightly different grid than the CMIP6 data - basically the static data just has longitudes that go from 0 to 360, and the CMIP6 data has longitudes from -180 to 180. Otherwise the grids are identical, which makes it easy to line them back up. To do that we just roll the longitude dimension by half of the total number of gridcells. We will assign the correct coordinates simply using the `assign_coords` method and set the new longitudes using CMIP6 data we just opened up. And then finally we can merge it all into a single `ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c40af92-1092-41f5-9243-5fe35e0cf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = get_static_data()\n",
    "static_data = static_data.roll(lon=720)\n",
    "static_data = static_data.assign_coords({'lon': met_ds['lon']})\n",
    "ds = xr.merge([met_ds.squeeze(), static_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b7e18-eb5c-4a54-8daa-23e305ef7eaf",
   "metadata": {},
   "source": [
    "You may note that the given `ds` here is still the global data, so now we use the `select_region` method to clip out just the region(s) that we specified in the configuration. Next, is another preprocessing step for making the CMIP6 data match the ERA5 training data by scaling the precipitation from mm/day to mm/second. This is a simple division by the number of seconds in a day, 86400. Then, we copy out the mask just like we did in the prevous section of the tutorial and set the mask in the test data to be all ones so we can run the forward mode on the full domain all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4a3f2-b071-4f64-a7cf-1d7c7d02ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = select_region(ds, config['data_config']['regions'])\n",
    "# Need to convert from mm/day to mm/s to match ERA5\n",
    "test_data['pr'] = test_data['pr'] / 86400\n",
    "# Put in the dummy mask and record the true mask\n",
    "true_mask = test_data['mask'].copy()\n",
    "test_data = test_data.fillna(1.0)\n",
    "test_data['mask'].values[:] = 1.0\n",
    "test_data = test_data.fillna(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21367ea6-43e2-45e3-b56d-68984f8417b7",
   "metadata": {},
   "source": [
    "Next up, we just have to perform the same tricks we used during inference last time by recording the actual shape of the data and then telling the config that we're running the whole domain per batch by setting the `batch_dims`. Last, but not least, we set the `output_var` to be `pr` since we don't acutally have `swe` in the CMIP data and have to have something there. With that all set we can make the data pipeline and get to running the model!\n",
    "\n",
    "To make our analysis tractable we'll then subset it down to three time periods:\n",
    "* 2015-2035: We will call this \"2020s\"\n",
    "* 2040-2060: We will call this \"2050s\"\n",
    "* 2070-2090: We will call this \"2080s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c2d4d-48c3-4c4a-a785-0ec52743e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_pred = run_model(model, test_data, config)\n",
    "swe_2020s = swe_pred.sel(time=slice('2015', '2035'))\n",
    "swe_2050s = swe_pred.sel(time=slice('2040', '2060'))\n",
    "swe_2080s = swe_pred.sel(time=slice('2070', '2090'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d0bc4-0a84-4452-9f67-f492f0117d6c",
   "metadata": {},
   "source": [
    "### Analyzing the projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c30b9-9353-4119-885f-40ccda36fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_wateryear(ds):\n",
    "    result = (ds.time.dt.dayofyear + 92 - 1 ) % (365 + ds.time.dt.is_leap_year) + 1\n",
    "    result.name = 'dowy'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa21f0-c70a-413c-9ec5-3804cb4a2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quantile_spread(da, quantiles, ax):\n",
    "    da_wy = da.groupby(day_of_wateryear(da)).quantile(quantiles)\n",
    "    dowy = da.dowy\n",
    "    ax.fill_between(\n",
    "        dowy, da_wy.sel(quantile=quantiles[0]), da_wy.sel(quantile=quantiles[-1]),  \n",
    "        alpha=0.25, color='grey'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        dowy, da_wy.sel(quantile=quantiles[1]), da_wy.sel(quantile=quantiles[-2]),  \n",
    "        alpha=0.25, color='grey'\n",
    "    )\n",
    "    ax.plot(dowy, da_wy.sel(quantile=quantiles[2]), color='grey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278a5ff-1e18-4b1d-9705-24808e202f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = {'lat': slice(38, 40,), 'lon': slice(252-360, 254-360)}\n",
    "swe_loc_20 = swe_2020s.sel(**loc).mean(dim=['lat', 'lon'])\n",
    "swe_loc_50 = swe_2050s.sel(**loc).mean(dim=['lat', 'lon'])\n",
    "swe_loc_80 = swe_2080s.sel(**loc).mean(dim=['lat', 'lon'])\n",
    "\n",
    "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4), sharey=True)\n",
    "plot_quantile_spread(swe_loc_20, quantiles, axes[0])\n",
    "axes[0].set_title('2020s')\n",
    "plot_quantile_spread(swe_loc_50, quantiles, axes[1])\n",
    "axes[1].set_title('2050s')\n",
    "plot_quantile_spread(swe_loc_80, quantiles, axes[2])\n",
    "axes[2].set_title('2080s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fbcaa4-143a-41ac-8283-c4e3448df4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes[0].set_ylabel('SWE [m]')\n",
    "axes[1].set_xlabel('Day of Wateryear')\n",
    "plt.suptitle('Southern Rockies', fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f247a-d6d8-4b3b-bf0a-3b6c1fc1ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_q_20 = swe_loc_20.groupby(day_of_wateryear(swe_loc_20)).quantile(quantiles)\n",
    "swe_q_50 = swe_loc_50.groupby(day_of_wateryear(swe_loc_50)).quantile(quantiles)\n",
    "swe_q_80 = swe_loc_80.groupby(day_of_wateryear(swe_loc_80)).quantile(quantiles)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4), sharey=True)\n",
    "dowy = swe_q_20.dowy\n",
    "axes[0].fill_between(dowy, swe_q_20.sel(quantile=quantiles[0]), swe_q_20.sel(quantile=quantiles[-1]),  alpha=0.25, color='grey')\n",
    "axes[0].fill_between(dowy, swe_q_20.sel(quantile=quantiles[1]), swe_q_20.sel(quantile=quantiles[-2]),  alpha=0.25, color='grey')\n",
    "axes[0].plot(dowy, swe_q_20.sel(quantile=quantiles[2]), color='grey')\n",
    "axes[0].set_title('2020s')\n",
    "\n",
    "axes[1].fill_between(dowy, swe_q_50.sel(quantile=quantiles[0]), swe_q_50.sel(quantile=quantiles[-1]),  alpha=0.25, color='grey')\n",
    "axes[1].fill_between(dowy, swe_q_50.sel(quantile=quantiles[1]), swe_q_50.sel(quantile=quantiles[-2]),  alpha=0.25, color='grey')\n",
    "axes[1].plot(dowy, swe_q_50.sel(quantile=quantiles[2]), color='grey')\n",
    "axes[1].set_title('2050s')\n",
    "\n",
    "axes[2].fill_between(dowy, swe_q_80.sel(quantile=quantiles[0]), swe_q_80.sel(quantile=quantiles[-1]),  alpha=0.25, color='grey')\n",
    "axes[2].fill_between(dowy, swe_q_80.sel(quantile=quantiles[1]), swe_q_80.sel(quantile=quantiles[-2]),  alpha=0.25, color='grey')\n",
    "axes[2].plot(dowy, swe_q_80.sel(quantile=quantiles[2]), color='grey')\n",
    "axes[2].set_title('2080s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d287e-441d-4048-beb5-026eebeb1823",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = {'lat': slice(47, 49), 'lon': slice(238-360, 240-360), }# 'method': 'nearest'}\n",
    "swe_loc_20 = swe_2020s.sel(**loc).mean(dim=['lat', 'lon'])\n",
    "swe_loc_50 = swe_2050s.sel(**loc).mean(dim=['lat', 'lon'])\n",
    "swe_loc_80 = swe_2080s.sel(**loc).mean(dim=['lat', 'lon'])\n",
    "\n",
    "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "swe_q_20 = swe_loc_20.groupby(day_of_wateryear(swe_loc_20)).quantile(quantiles)\n",
    "swe_q_50 = swe_loc_50.groupby(day_of_wateryear(swe_loc_50)).quantile(quantiles)\n",
    "swe_q_80 = swe_loc_80.groupby(day_of_wateryear(swe_loc_80)).quantile(quantiles)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4), sharey=True)\n",
    "dowy = swe_q_20.dowy\n",
    "axes[0].fill_between(dowy, swe_q_20.sel(quantile=quantiles[0]), swe_q_20.sel(quantile=quantiles[-1]),  alpha=0.25, color='grey')\n",
    "axes[0].fill_between(dowy, swe_q_20.sel(quantile=quantiles[1]), swe_q_20.sel(quantile=quantiles[-2]),  alpha=0.25, color='grey')\n",
    "axes[0].plot(dowy, swe_q_20.sel(quantile=quantiles[2]), color='grey')\n",
    "axes[0].set_title('2020s')\n",
    "\n",
    "axes[1].fill_between(dowy, swe_q_50.sel(quantile=quantiles[0]), swe_q_50.sel(quantile=quantiles[-1]),  alpha=0.25, color='grey')\n",
    "axes[1].fill_between(dowy, swe_q_50.sel(quantile=quantiles[1]), swe_q_50.sel(quantile=quantiles[-2]),  alpha=0.25, color='grey')\n",
    "axes[1].plot(dowy, swe_q_50.sel(quantile=quantiles[2]), color='grey')\n",
    "axes[1].set_title('2050s')\n",
    "\n",
    "axes[2].fill_between(dowy, swe_q_80.sel(quantile=quantiles[0]), swe_q_80.sel(quantile=quantiles[-1]),  alpha=0.25, color='grey')\n",
    "axes[2].fill_between(dowy, swe_q_80.sel(quantile=quantiles[1]), swe_q_80.sel(quantile=quantiles[-2]),  alpha=0.25, color='grey')\n",
    "axes[2].plot(dowy, swe_q_80.sel(quantile=quantiles[2]), color='grey')\n",
    "axes[2].set_title('2080s')\n",
    "\n",
    "axes[0].set_ylabel('SWE [m]')\n",
    "axes[1].set_xlabel('Day of Wateryear')\n",
    "plt.suptitle('Northern Cascades', fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c73c11-6458-4695-b9c2-e9d0374a9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = {'lat': slice( 37.5, 38.5,), 'lon': slice(239.75-360, 240.25-360, )}\n",
    "swe_loc_20 = swe_2020s.sel(**loc).mean(dim=['lat', 'lon'])\n",
    "swe_loc_50 = swe_2050s.sel(**loc).mean(dim=['lat', 'lon'])\n",
    "swe_loc_80 = swe_2080s.sel(**loc).mean(dim=['lat', 'lon'])\n",
    "\n",
    "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "swe_q_20 = swe_loc_20.groupby(day_of_wateryear(swe_loc_20)).quantile(quantiles)\n",
    "swe_q_50 = swe_loc_50.groupby(day_of_wateryear(swe_loc_50)).quantile(quantiles)\n",
    "swe_q_80 = swe_loc_80.groupby(day_of_wateryear(swe_loc_80)).quantile(quantiles)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4), sharey=True)\n",
    "dowy = swe_q_20.dowy\n",
    "axes[0].fill_between(dowy, swe_q_20.sel(quantile=quantiles[0]), swe_q_20.sel(quantile=quantiles[-1]),  alpha=0.25, color='grey')\n",
    "axes[0].fill_between(dowy, swe_q_20.sel(quantile=quantiles[1]), swe_q_20.sel(quantile=quantiles[-2]),  alpha=0.25, color='grey')\n",
    "axes[0].plot(dowy, swe_q_20.sel(quantile=quantiles[2]), color='grey')\n",
    "axes[0].set_title('2020s')\n",
    "\n",
    "axes[1].fill_between(dowy, swe_q_50.sel(quantile=quantiles[0]), swe_q_50.sel(quantile=quantiles[-1]),  alpha=0.25, color='grey')\n",
    "axes[1].fill_between(dowy, swe_q_50.sel(quantile=quantiles[1]), swe_q_50.sel(quantile=quantiles[-2]),  alpha=0.25, color='grey')\n",
    "axes[1].plot(dowy, swe_q_50.sel(quantile=quantiles[2]), color='grey')\n",
    "axes[1].set_title('2050s')\n",
    "\n",
    "axes[2].fill_between(dowy, swe_q_80.sel(quantile=quantiles[0]), swe_q_80.sel(quantile=quantiles[-1]),  alpha=0.25, color='grey')\n",
    "axes[2].fill_between(dowy, swe_q_80.sel(quantile=quantiles[1]), swe_q_80.sel(quantile=quantiles[-2]),  alpha=0.25, color='grey')\n",
    "axes[2].plot(dowy, swe_q_80.sel(quantile=quantiles[2]), color='grey')\n",
    "axes[2].set_title('2080s')\n",
    "\n",
    "axes[0].set_ylabel('SWE [m]')\n",
    "axes[1].set_xlabel('Day of Wateryear')\n",
    "plt.suptitle('Central Sierra Nevada', fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1623f-f1c6-4cf4-9780-d72577f59841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ed565-4e7a-418a-8e4d-e5b16bf0c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = pd.pivot_table(\n",
    "    df_20, \n",
    "    index=df.index.dayofyear, \n",
    "    columns=df.index.year,\n",
    "    values='Tuolumne Meadows Pillow SWE [mm]', \n",
    "    aggfunc='mean'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57dceb5-d00d-45a6-8474-13c2dee080f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "swe_seas_mean_2020s = swe_2020s.groupby(swe_2020s['time'].dt.season).mean()\n",
    "swe_seas_mean_2050s = swe_2050s.groupby(swe_2050s['time'].dt.season).mean()\n",
    "swe_seas_mean_2080s = swe_2080s.groupby(swe_2080s['time'].dt.season).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92227359-5f2b-43d8-8bf1-1b3ec42753b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm, SymLogNorm, PowerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf6b17-6390-4dd1-b3fa-36f6a37c8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "swe_seas_mean_2020s.sel(season='DJF').plot(norm=PowerNorm(gamma=0.5, vmin=1e-9, vmax=0.15), cmap='turbo', ax=axes[0], )\n",
    "swe_seas_mean_2050s.sel(season='DJF').plot(norm=PowerNorm(gamma=0.5, vmin=1e-9, vmax=0.15), cmap='turbo', ax=axes[1], )\n",
    "swe_seas_mean_2080s.sel(season='DJF').plot(norm=PowerNorm(gamma=0.5, vmin=1e-9, vmax=0.15), cmap='turbo', ax=axes[2], )\n",
    "plt.suptitle('Winter mean SWE [m]', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c25be-93fa-402a-affb-60a08dd00b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "swe_seas_mean_2020s.sel(season='MAM').plot(norm=PowerNorm(gamma=0.5, vmin=1e-9, vmax=0.2), cmap='turbo', ax=axes[0])\n",
    "swe_seas_mean_2050s.sel(season='MAM').plot(norm=PowerNorm(gamma=0.5, vmin=1e-9, vmax=0.2), cmap='turbo', ax=axes[1])\n",
    "swe_seas_mean_2080s.sel(season='MAM').plot(norm=PowerNorm(gamma=0.5, vmin=1e-9, vmax=0.2), cmap='turbo', ax=axes[2])\n",
    "plt.suptitle('Spring mean SWE [m]', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d99c13-5a69-4550-9693-ac5cc48c89d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
