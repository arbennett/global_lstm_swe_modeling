{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb35da6-0e5a-40d1-929f-b9416e21944f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial 3: Evaluating the trained model on the test data\n",
    "\n",
    "### Outline\n",
    "\n",
    "* Imports, including library code from previous steps\n",
    "* Loading the trained model using hyperparameters and weights file\n",
    "* Setting up the datapipe for the test data\n",
    "* Some functions for \"undoing/inverting\" the ETL pipeline (aka recovering spatiotemporal relations)\n",
    "* Running the trained model in eval mode\n",
    "* Some basic metrics and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e864d7-2742-4aee-8c49-ae8ff0500bf2",
   "metadata": {},
   "source": [
    "## Setup and configuration\n",
    "\n",
    "As you might expect, we start with some standard imports. Here we will be also importing our ability to create LSTM based models and load experiments which fit our narrow focus via the `src.models` and `src.utils` modules which you can find included with the tutorial. Additionally we are using the `src.datapipes` module which comes from earlier as well. Finally, we set the device and data type as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc651d5a-1941-4746-8901-b0994ae6b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from src.models import create_lstm_model\n",
    "from src.utils import load_experiment\n",
    "from src.datapipes import make_data_pipeline, merge_data, select_region\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c912a1d8-5311-4a14-a3f8-ecba5690bc22",
   "metadata": {},
   "source": [
    "## Loading the setup from our saved experiment\n",
    "\n",
    "With the defaults out of the way, we can prove how nice it is to have even some minimal MLOps infrastructure set up via the `save_experiment` and `load_experiment` functions by simply loading up the previously saved experiment. We then instantiate an equivalent model structure and load the trained model into it. We are then ready to start thinking about how we can apply this model to new data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5579e1-6ba5-41d7-9c4b-077418534ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '../experiments/tutorial/tutorial.yml'\n",
    "config = load_experiment(config_file)\n",
    "model = create_lstm_model(**config['model_config'])\n",
    "model.load_state_dict(torch.load(config['weights_file'], map_location=DEVICE))\n",
    "model.to(DTYPE).to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8952753-6f23-4e68-94e9-696dad451162",
   "metadata": {},
   "source": [
    "## Data plumbing for model inference\n",
    "\n",
    "We expect you're getting excited at this point to see how the trained model performs, but the unfortunate reality of using deep-learning models means that there is often a rift between training and application data workflows. In our case it's not to onerous, but it does require a little bit of work to be most efficient. As always, we just want to open the data up as a first step, and we can reuse previous data processing functions to get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496f474-3a1b-4f39-91bc-ab5bab15a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = merge_data()\n",
    "test_data = select_region(\n",
    "    ds.sel(time=config['data_config']['test_period']),\n",
    "    config['data_config']['regions']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50341e-5f9e-4ce3-8c48-a7376e1b88cd",
   "metadata": {},
   "source": [
    "Beyond that, we don't wan't to filter out missing data because that would mean we have ragged arrays that need complex logic to reconstruct. Instead we can just fill missing data. On one hand, this might seem like a hack simply to reduce the number of lines of code, but on the other is actually a nice optimization because of how fast these types of trained models can run compared to actually solving differential equations. That is, it's easier to run a forward pass on some irrelevant data than it is to activiely filter out the missing regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f70e21-3096-4363-8bf1-ab7c3b02d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mask = test_data['mask'].copy()\n",
    "test_data = test_data.fillna(1.0)\n",
    "test_data['mask'].values[:] = 1.0\n",
    "test_data = test_data.fillna(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162e6e4-0f66-4b6d-b6a8-7343b0a7aca1",
   "metadata": {},
   "source": [
    "Now, with the \"filled\" data, we want to make sure that we record the actual dimensions of the data as a tuple of `(lat, lon, time)` so that we can reconstruct things later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf7bd9-95c4-4f82-899c-7415d414ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_shape = (\n",
    "    len(test_data['lat']),\n",
    "    len(test_data['lon']),\n",
    "    config['data_config']['output_sequence_length']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee5eea-87d4-4059-a7f7-ae3d53cf5bb5",
   "metadata": {},
   "source": [
    "Now we can record the dimensions that we'll iterate over in place over the onces that we used at train time. What we are doing here is simply setting the `batch_dims` keyword that will go into the `make_data_pipeline` function that we developed in the `datapipes` module. Effectively all this says is to run the full domain on every forward pass of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d4a82-ea1e-4073-bb92-fc106cc00444",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['data_config']['batch_dims'] = {\n",
    "    'lat': len(test_data['lat']),\n",
    "    'lon': len(test_data['lon'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396cc1ca-6e88-44b9-a113-e9bae153356e",
   "metadata": {},
   "source": [
    "And finally, we make the data pipeline with our same old `make_data_pipeline` function. We set a few extra parameters like `min_samples=0` so we don't filter anywhere out, `preload=True` so we load the data automatically to save computational cost, and `filter_mask=False` to include data outside of the masked region in order to make spatiotemporally complete predictions. The rest of the configuration comes from what we recorded in the `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ab2e6-fb2c-4149-9019-aff2ec6f0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_data_pipeline(\n",
    "    ds=test_data, \n",
    "    min_samples=0, \n",
    "    preload=True,\n",
    "    filter_mask=False,\n",
    "    **config['data_config']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6553ace-b5d2-4215-aa73-c7c8adeb7e84",
   "metadata": {},
   "source": [
    "## Running the model in forward/inference mode\n",
    "\n",
    "Given this, we are ready to actually run our trained model on the testing dataset. For this we'll loop over the new `pipe` object. For every element in the data pipe we can simply transfer it onto the `DEVICE` and run it through the model. In the process we have specified `with torch.no_grad()` which ensures that we do not run the backwards pass on the model, saving computation. We also make sure to transfer the predictions back onto the CPU from whatever `DEVICE` they were run on and finally reshape everything back into the correct shape, which basically unflattens things so that the spatial relations are recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82692bfe-b984-4cb1-9c86-f6446ae6f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i, (x, y) in tqdm(enumerate(pipe)):\n",
    "    x = x.to(DTYPE).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        yhat = model(x).cpu().float()\n",
    "    yhat = yhat.reshape(actual_shape)\n",
    "    predictions.append(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f342f-ef3c-4d1b-ad08-808f93c002dd",
   "metadata": {},
   "source": [
    "## Putting the data back together\n",
    "With the above cell we've done most of the work, but it's often convenient to repackage the resulting predictions from a stack of numpy arrays into an xarray Dataset so that we can easily locate the data in time/space coordinates that are easily interpreted by humans. To do so we can simply make use of the \"coordinates\" from the truth/reference dataset from ERA5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc1ec7-10ea-4af4-b177-bff6cf57c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = config['data_config']['input_overlap']['time'] \n",
    "swe_true = test_data['swe'].isel(time=slice(start_time, -7))\n",
    "\n",
    "swe_pred = xr.DataArray(\n",
    "    torch.concat(predictions, dim=2).squeeze().cpu(),\n",
    "    dims=('lat', 'lon', 'time'), coords = swe_true.coords\n",
    ") \n",
    "swe_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8442b3-33ea-400d-88b3-66167a708f36",
   "metadata": {},
   "source": [
    "## Getting to the analysis and quantifying model performance\n",
    "\n",
    "At this point we've assembled all of the predictions and reference data into similar data formats and all that's left for us to do is some analysis. Model analysis very problem and domain specific, but we will cover some basic analytics on our model here. First, if you have applied your model to the `WNA`, or Western North America region, you will see we've picked out some individual regions to look at individual timeseries on the spatial averages. These include major portions of prominent mountain ranges such as the Southern Rocky Mountains, Northern Cascade Mountains, and Central Sierra Nevada Mountains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15acb5e4-0fe0-43a2-a3d4-8e967fbe0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = {'lat': slice(40, 38), 'lon': slice(252, 254), }\n",
    "swe_true.sel(**loc).mean(dim=['lat', 'lon']).plot()\n",
    "(3 * swe_pred).sel(**loc).mean(dim=['lat', 'lon']).plot()\n",
    "plt.title('Southern Rockies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3eebbd-af99-44bf-ac1a-a77f2a2bb4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = {'lat': slice(49, 47), 'lon': slice(238, 240), }# 'method': 'nearest'}\n",
    "swe_true.sel(**loc).mean(dim=['lat', 'lon']).plot()\n",
    "(3 * swe_pred).sel(**loc).mean(dim=['lat', 'lon']).plot()\n",
    "plt.title('Northern Cascades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f86d8-bb18-44d2-aea7-7309ea5579e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = {'lat': slice(38.5, 37.5), 'lon': slice(239.75, 240.25), }# 'method': 'nearest'}\n",
    "swe_true.sel(**loc).mean(dim=['lat', 'lon']).plot()\n",
    "(3 * swe_pred).sel(**loc).mean(dim=['lat', 'lon']).plot()\n",
    "plt.title('Central Sierra Nevada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d6641-364b-4fba-aee9-052b2d874e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_pred.isel(time=slice(120, 400)).max(dim='time').plot(vmax=0.1, vmin=0, cmap='turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee36eb02-28a9-4ef6-b492-47b4e2d4ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_true.isel(time=slice(120, 400)).max(dim='time').plot(vmax=0.1, vmin=0, cmap='turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e96fc8-d0e1-4a2d-9a4a-e84c73139700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
