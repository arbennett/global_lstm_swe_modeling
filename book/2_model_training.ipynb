{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Implementing the model and training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3PqH0Sr7myM",
    "outputId": "425949ac-3264-4144-d12d-220737c47422",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -q zarr torchdata zen3geo dask[distributed] intake xarray fsspec aiohttp regionmask --upgrade\n",
    "#!pip install -q git+https://github.com/carbonplan/cmip6-downscaling.git@1.0\n",
    "#!pip install -q git+https://github.com/xarray-contrib/xbatcher.git@463546e7739e68b10f1ae456fb910a1628de1e5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670556560147
    },
    "id": "EyDDFpFn7z9o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "import time\n",
    "import torch\n",
    "import torchdata\n",
    "import intake\n",
    "import regionmask\n",
    "import xbatcher\n",
    "import zen3geo as zg\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from torch import nn\n",
    "from tqdm.autonotebook import tqdm\n",
    "from functools import partial\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from torchdata.datapipes import functional_datapipe\n",
    "from torchdata.datapipes.iter import IterDataPipe\n",
    "from torchdata.datapipes.utils import StreamWrapper\n",
    "from torchdata.dataloader2 import DataLoader2\n",
    "from torch.utils.data import DataLoader\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datapipes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670555464985
    },
    "id": "ohF8jFzw8OdG"
   },
   "outputs": [],
   "source": [
    "class LSTMOutput(nn.Module):\n",
    "    def __init__(self, out_len=1):\n",
    "        super().__init__()\n",
    "        self.out_len = out_len\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # A stupid hack to get around the fact that nn.LSTM \n",
    "        # returns (output, (hn, cn))\n",
    "        # Output shape (batch, sequence_length, hidden)\n",
    "        tensor, _ = x\n",
    "        # Now just grab the last index on the sequence lenght\n",
    "        # Reshape shape (batch, hidden)\n",
    "        return tensor[:, -self.out_len:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1670557797268
    },
    "id": "I3CoB3HW8QdX",
    "outputId": "9f7a7817-bfd4-48ca-e22c-adbee29846b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = merge_data()\n",
    "in_vars = ['pr',  'tasmax',  'tasmin',  'elevation',  'aspect_cosine']\n",
    "out_vars = ['swe']\n",
    "varlist = ['mask'] + in_vars + out_vars\n",
    "input_sequence_length = 180  \n",
    "output_sequence_length = 1\n",
    "output_selector = {'time': slice(-output_sequence_length, None)}\n",
    "input_dims={'time': input_sequence_length}\n",
    "batch_dims={'lat': 30, 'lon': 30}\n",
    "input_overlap={'time': 45} \n",
    "\n",
    "convert = partial(\n",
    "    stack_split_convert, \n",
    "    in_vars=in_vars, \n",
    "    out_vars=out_vars, \n",
    "    out_selectors=output_selector,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1670555328046
    }
   },
   "outputs": [],
   "source": [
    "region = ['EEU']\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "dropout = 0.25\n",
    "train_period = slice('1985', '2015')\n",
    "base_name = f'regional_{'-'.join(regions)}_lstm_h{hidden_size}_d{num_layers}'\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.LSTM(\n",
    "        input_size=len(in_vars), \n",
    "        hidden_size=hidden_size, \n",
    "        batch_first=True,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "    ),\n",
    "    LSTMOutput(output_sequence_length),\n",
    "    nn.Linear(in_features=hidden_size, out_features=len(out_vars)),\n",
    "    nn.SELU()\n",
    ").float()\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fun = nn.MSELoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = RegionalSubsetterPipe(\n",
    "    ds[varlist].sel(time=train_period).astype(np.float32),\n",
    "    selected_regions=regions,\n",
    ")\n",
    "dp = dp.slice_with_xbatcher(\n",
    "    input_dims=input_dims,\n",
    "    batch_dims=batch_dims,\n",
    "    input_overlap=input_overlap,\n",
    "    preload_batch=False\n",
    ")\n",
    "dp = dp.map(filter_batch)\n",
    "dp = dp.map(transform_batch)\n",
    "dp = dp.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, datapipe, loss_fun, optimizer):\n",
    "    tot_loss = 0.0\n",
    "    for i, (x, y) in tqdm(enumerate(dp)):\n",
    "        if not len(x): continue\n",
    "        opt.zero_grad()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fun(yhat, y)\n",
    "        if not np.isnan(loss.cpu().detach().numpy()):\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_loss += loss.cpu().detach().numpy()\n",
    "    return tot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss = []\n",
    "max_epochs = 20\n",
    "for e in tqdm(range(max_epochs)):\n",
    "    loss = train_epoch(model, dp, loss_fun, opt)\n",
    "    torch.save(\n",
    "        model.state_dict(), \n",
    "        f'./logging/model_checkpoints/pt_files/{base_name}_e{e+starting_epoch:04}.pt'\n",
    "    )\n",
    "    with open(f'./logging/loss_{base_name}.txt', 'a') as f:\n",
    "        f.writelines([f'{loss}\\n'])\n",
    "    starting_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../logging/loss_{base_name}.txt', 'r') as f:\n",
    "     txt = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_loss = np.array([float(t) for t in txt])\n",
    "plt.plot(stack_loss[15:])\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
